{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a830bb4b",
   "metadata": {},
   "source": [
    "### 1. Train an AdaBoost Classifier on a sample dataset and print model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(\"AdaBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51721a",
   "metadata": {},
   "source": [
    "### 2. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "ada_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
    "ada_reg.fit(X_train, y_train)\n",
    "y_pred_reg = ada_reg.predict(X_test)\n",
    "print(\"AdaBoost Regressor MAE:\", mean_absolute_error(y_test, y_pred_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d488d7",
   "metadata": {},
   "source": [
    "### 3. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7940bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Classifier Feature Importance:\", gb_clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e9a94",
   "metadata": {},
   "source": [
    "### 4. Train a Gradient Boosting Regressor and evaluate using R-Squared Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c656496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=50, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "y_pred_gb_reg = gb_reg.predict(X_test)\n",
    "print(\"Gradient Boosting Regressor R2 Score:\", r2_score(y_test, y_pred_gb_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8eddf7",
   "metadata": {},
   "source": [
    "### 5. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64787337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=50, random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(\"XGBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471fff12",
   "metadata": {},
   "source": [
    "### 6. Train a CatBoost Classifier and evaluate using F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a369bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cat_clf = CatBoostClassifier(iterations=50, verbose=0)\n",
    "cat_clf.fit(X_train, y_train)\n",
    "y_pred_cat = cat_clf.predict(X_test)\n",
    "print(\"CatBoost Classifier F1-Score:\", f1_score(y_test, y_pred_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43040dc6",
   "metadata": {},
   "source": [
    "### 7. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=50, random_state=42)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred_xgb_reg = xgb_reg.predict(X_test)\n",
    "print(\"XGBoost Regressor MSE:\", mean_squared_error(y_test, y_pred_xgb_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd10094",
   "metadata": {},
   "source": [
    "### 8. Train an AdaBoost Classifier and visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa316f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(X.shape[1]), ada_clf.feature_importances_)\n",
    "plt.title(\"AdaBoost Classifier Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254808c",
   "metadata": {},
   "source": [
    "### 9. Train a Gradient Boosting Regressor and plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for m in range(1, len(X_train)):\n",
    "    gb_reg.fit(X_train[:m], y_train[:m])\n",
    "    y_train_predict = gb_reg.predict(X_train[:m])\n",
    "    y_test_predict = gb_reg.predict(X_test)\n",
    "    train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
    "\n",
    "plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Training error\")\n",
    "plt.plot(np.sqrt(test_errors), \"b-\", linewidth=2, label=\"Testing error\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient Boosting Regressor Learning Curves\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a951b4",
   "metadata": {},
   "source": [
    "### 10. Train an XGBoost Classifier and visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4919f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = xgb_clf.feature_importances_\n",
    "plt.bar(range(len(importances)), importances)\n",
    "plt.title(\"XGBoost Classifier Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2641c2c",
   "metadata": {},
   "source": [
    "### 11. Train a CatBoost Classifier and plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b075da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_cat)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title(\"CatBoost Classifier Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d974a2e",
   "metadata": {},
   "source": [
    "### 12. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimators = [10, 50, 100, 200]\n",
    "accuracies = []\n",
    "\n",
    "for n in estimators:\n",
    "    model = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.plot(estimators, accuracies, marker='o')\n",
    "plt.title(\"AdaBoost Accuracy with Different Estimators\")\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f443b65",
   "metadata": {},
   "source": [
    "### 13. Train a Gradient Boosting Classifier and visualize the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ad853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_prob = gb_clf.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient Boosting Classifier ROC Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4c14c",
   "metadata": {},
   "source": [
    "### 14. Train an XGBoost Regressor and tune the learning rate using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23532abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
    "grid_search = GridSearchCV(XGBRegressor(n_estimators=50), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best learning rate:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0af12",
   "metadata": {},
   "source": [
    "### 15. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_clf_balanced = CatBoostClassifier(iterations=50, class_weights=[1, 5], verbose=0)\n",
    "cat_clf_balanced.fit(X_train, y_train)\n",
    "y_pred_balanced = cat_clf_balanced.predict(X_test)\n",
    "print(\"Balanced CatBoost Classifier F1-Score:\", f1_score(y_test, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d922d65",
   "metadata": {},
   "source": [
    "### 16. Train an AdaBoost Classifier and analyze the effect of different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rates = [0.01, 0.1, 0.5, 1]\n",
    "for lr in learning_rates:\n",
    "    model = AdaBoostClassifier(n_estimators=50, learning_rate=lr, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Learning rate {lr}: Accuracy = {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8272668",
   "metadata": {},
   "source": [
    "### 17. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_digits, y_digits, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_multi = XGBClassifier(n_estimators=50, random_state=42)\n",
    "xgb_multi.fit(X_train_d, y_train_d)\n",
    "y_prob_multi = xgb_multi.predict_proba(X_test_d)\n",
    "print(\"XGBoost Multi-class Log-Loss:\", log_loss(y_test_d, y_prob_multi))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
